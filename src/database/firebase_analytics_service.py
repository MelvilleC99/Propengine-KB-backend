"""
Firebase Analytics Service - Handles writing query analytics to Firestore
"""

import logging
from typing import Dict, List, Optional
from google.cloud.firestore_v1 import SERVER_TIMESTAMP

from src.database.firebase_client import get_firestore_client

logger = logging.getLogger(__name__)


class FirebaseAnalyticsService:
    """Service for writing KB query analytics to Firebase"""
    
    def __init__(self):
        self.db = get_firestore_client()
        self.analytics_collection = "kb_analytics"
    
    async def batch_write_analytics(
        self, 
        session_id: str, 
        agent_id: str, 
        queries: List[Dict],
        session_costs: Optional[Dict] = None
    ) -> bool:
        """
        Write all query analytics in ONE batch at session end
        
        Args:
            session_id: Session ID
            agent_id: Agent ID
            queries: List of query metadata dicts
            session_costs: Optional cost breakdown from token_tracker
        """
        try:
            if not self.db or not queries:
                return False
            
            # Use batch write for efficiency
            batch = self.db.batch()
            
            # Get session-level costs (will be distributed across queries)
            operations_costs = {}
            if session_costs:
                operations_costs = session_costs.get("operations", {})
            
            num_queries = len(queries)
            
            for idx, query_data in enumerate(queries):
                # Generate unique query ID (auto-generated by Firestore)
                query_ref = self.db.collection(self.analytics_collection).document()
                
                # Extract KB entries used
                kb_entries_used = []
                sources_used = query_data.get("sources_used", [])
                
                # Format sources as array of entry information
                for source in sources_used:
                    entry_info = {
                        "entry_id": source.get("entry_id"),  # AstraDB chunk ID
                        "parent_entry_id": source.get("parent_entry_id"),  # ← Firebase KB entry ID
                        "entry_title": source.get("metadata", {}).get("title") or source.get("title", "Unknown"),
                        "entry_type": source.get("entry_type", "unknown"),
                        "similarity_score": source.get("similarity_score", 0.0)
                    }
                    kb_entries_used.append(entry_info)
                
                analytics_doc = {
                    "query_id": query_ref.id,  # Auto-generated document ID
                    "session_id": session_id,
                    "agent_id": agent_id,
                    "timestamp": query_data.get("timestamp", SERVER_TIMESTAMP),
                    
                    # Query details
                    "query_text": query_data.get("query_text"),
                    "query_type": query_data.get("query_type"),
                    "category": query_data.get("category"),
                    
                    # KB entries used in this query (includes parent_entry_id!)
                    "kb_entries_used": kb_entries_used,  # ← Array with parent_entry_id
                    
                    # Performance metrics
                    "confidence_score": query_data.get("confidence_score", 0.0),
                    "sources_found": query_data.get("sources_found", 0),
                    "response_time_ms": query_data.get("response_time_ms", 0),
                    
                    # Token usage and costs (NEW!)
                    "costs": operations_costs if operations_costs else None,
                    
                    # Outcomes
                    "escalated": query_data.get("escalated", False),
                    "user_feedback": query_data.get("user_feedback")  # from thumbs up/down
                }
                
                batch.set(query_ref, analytics_doc)
            
            # Commit batch
            batch.commit()
            logger.info(f"✅ Batch wrote {len(queries)} analytics docs for session {session_id}")
            return True
            
        except Exception as e:
            logger.error(f"❌ Failed to batch write analytics: {e}")
            return False
